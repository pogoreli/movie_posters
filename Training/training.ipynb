{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1qIbOiuMZjBEqyaH_hxp-e1IuE1XLQ7rl","authorship_tag":"ABX9TyMkMx+0dU+ctq1FvrPvvExn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":43,"metadata":{"id":"DfC8Gks9b_eG","executionInfo":{"status":"ok","timestamp":1732584880242,"user_tz":300,"elapsed":269,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"outputs":[],"source":["# ===========================\n","#        Imports\n","# ===========================\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n","from torchvision import transforms, models\n","from google.colab import drive\n","import pandas as pd\n","import io\n","import numpy as np\n","from sklearn.metrics import f1_score, precision_score, recall_score, classification_report"]},{"cell_type":"code","source":["# ===========================\n","#    Mount Google Drive\n","# ===========================\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSP5q_NKibkI","executionInfo":{"status":"ok","timestamp":1732584883201,"user_tz":300,"elapsed":2675,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"outputId":"72a85cc2-b83d-42c1-8188-52335418b34d"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ===========================\n","#    Dataset Definition\n","# ===========================\n","class ParquetDataset(Dataset):\n","    def __init__(self, parquet_file_or_df, transform=None):\n","        \"\"\"\n","        Custom Dataset class to load images and genre labels from a Parquet file or DataFrame.\n","\n","        Parameters:\n","            parquet_file_or_df (str or pd.DataFrame): Path to the Parquet file or a DataFrame.\n","            transform (callable, optional): Optional transform to be applied on an image.\n","        \"\"\"\n","        if isinstance(parquet_file_or_df, str):\n","            # Load from parquet file if a string path is provided\n","            self.df = pd.read_parquet(parquet_file_or_df)\n","        elif isinstance(parquet_file_or_df, pd.DataFrame):\n","            # Use the DataFrame directly if it is provided\n","            self.df = parquet_file_or_df\n","        else:\n","            raise ValueError(\"parquet_file_or_df must be either a file path (str) or a pandas DataFrame.\")\n","\n","        self.transform = transform\n","\n","        # Identify genre columns by excluding known columns\n","        self.non_genre_columns = ['movie_id', 'movie_name', 'movie_poster']\n","        self.genre_columns = self.df.columns.drop(self.non_genre_columns)\n","        self.num_genres = len(self.genre_columns)\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves the image and genre labels at the specified index.\n","\n","        Parameters:\n","            idx (int): Index of the data point.\n","\n","        Returns:\n","            image (Tensor): The transformed image tensor.\n","            genres (Tensor): The genre labels as a float tensor.\n","        \"\"\"\n","        row = self.df.iloc[idx]\n","\n","        # Load image from binary data\n","        image_binary = row['movie_poster']\n","        image = self.load_image_from_binary(image_binary)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Extract genre labels and convert to tensor\n","        genres = row[self.genre_columns].values.astype(np.float32)\n","        genres = torch.from_numpy(genres)\n","\n","        return image, genres\n","\n","    @staticmethod\n","    def load_image_from_binary(image_binary):\n","        \"\"\"\n","        Converts binary image data to a PIL Image.\n","\n","        Parameters:\n","            image_binary (bytes): Binary image data.\n","\n","        Returns:\n","            image (PIL Image): The loaded image in RGB format.\n","        \"\"\"\n","        img_byte_arr = io.BytesIO(image_binary)\n","        image = Image.open(img_byte_arr).convert(\"RGB\")\n","        return image\n"],"metadata":{"id":"nyKZ7lHFcvoc","executionInfo":{"status":"ok","timestamp":1732584883202,"user_tz":300,"elapsed":5,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#    Data Preparation\n","# ===========================\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","dataset_path = '/content/drive/MyDrive/Projects/movie_posters/Training/dataset.parquet'\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Initialize dataset\n","parquet_dataset = ParquetDataset(dataset_path, transform=transform)\n","\n","# Genre distribution analysis\n","genre_counts = parquet_dataset.df[parquet_dataset.genre_columns].sum()\n","print(\"Genre Counts:\\n\", genre_counts.sort_values(ascending=False))\n","\n","# Class weights for imbalanced dataset\n","class_counts = genre_counts.values\n","# Add a small epsilon to avoid divide by zero errors\n","epsilon = 1e-6\n","# class_weights = 1.0 / (class_counts + epsilon)  # Inverse frequency\n","# class_weights /= class_weights.sum()  # Normalize class weights to ensure the sum is 1\n","# class_weights = torch.tensor(class_weights, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Calculate the number of negative samples for each class\n","neg_counts = len(parquet_dataset) - class_counts\n","\n","# Compute pos_weight as the ratio of negative to positive samples\n","class_weights = neg_counts / (class_counts + epsilon)\n","class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n","\n","# Handle rare `labels_sum` groups by ensuring at least 2 samples per group\n","labels_sum = parquet_dataset.df[parquet_dataset.genre_columns].sum(axis=1)\n","\n","while True:\n","    rare_labels = labels_sum.value_counts()[labels_sum.value_counts() < 2].index\n","\n","    if len(rare_labels) == 0:  # Exit loop if no rare groups remain\n","        break\n","\n","    # Duplicate samples for rare groups until they have at least 2 samples\n","    for rare_label in rare_labels:\n","        rare_samples = parquet_dataset.df[labels_sum == rare_label]\n","        # Append rare samples to the dataframe until each group has at least 2 samples\n","        parquet_dataset.df = pd.concat([parquet_dataset.df, rare_samples], ignore_index=True)\n","\n","    # Recalculate labels_sum after each iteration to ensure all rare labels are handled\n","    labels_sum = parquet_dataset.df[parquet_dataset.genre_columns].sum(axis=1)\n","\n","print(f\"Dataset length after duplication: {len(parquet_dataset.df)}\")\n","\n","# Recompute `labels_sum` directly from the updated dataset\n","labels_sum = parquet_dataset.df[parquet_dataset.genre_columns].sum(axis=1)\n","print(f\"Recomputed labels_sum:\\n{labels_sum.value_counts()}\")  # Verify no group has < 2 samples\n","\n","# Perform stratified split using the updated DataFrame\n","from sklearn.model_selection import train_test_split\n","\n","train_idx, val_idx = train_test_split(\n","    range(len(parquet_dataset.df)),  # Ensure consistent length with dataset\n","    test_size=0.2,\n","    stratify=labels_sum,\n","    random_state=42\n",")\n","\n","# Create Dataset instances from the split DataFrames\n","train_dataset_df = parquet_dataset.df.iloc[train_idx].reset_index(drop=True)\n","val_dataset_df = parquet_dataset.df.iloc[val_idx].reset_index(drop=True)\n","\n","train_dataset = ParquetDataset(train_dataset_df, transform=transform)\n","val_dataset = ParquetDataset(val_dataset_df, transform=transform)\n","\n","# Weighted sampler for training\n","labels_matrix = train_dataset.df[train_dataset.genre_columns].values\n","sample_weights = np.dot(labels_matrix, class_weights.cpu().numpy())\n","# Ensure all weights are positive, add epsilon to handle zero weights\n","sample_weights = np.clip(sample_weights, a_min=epsilon, a_max=None)\n","\n","sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n","\n","# Data loaders\n","train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler, num_workers=os.cpu_count())\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n","\n","# Debug DataLoader\n","for images, genres in train_loader:\n","    print(f\"Batch - Images: {images.shape}, Genres: {genres.shape}\")\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIBREeJ57nL1","executionInfo":{"status":"ok","timestamp":1732584884306,"user_tz":300,"elapsed":1108,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"outputId":"868a4d19-d167-4d71-8162-1dd82cdc9ce1"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Genre Counts:\n"," Drama                 765\n","Thriller              244\n","Comedy                231\n","Crime                 201\n","Action                199\n","                     ... \n","Seinen                  1\n","Jukebox Musical         1\n","Bumbling Detective      1\n","Sketch Comedy           1\n","Shōjo                   1\n","Length: 161, dtype: int64\n","Dataset length after duplication: 995\n","Recomputed labels_sum:\n","4     177\n","5     161\n","3     139\n","6     127\n","2     101\n","7      94\n","10     62\n","8      61\n","9      39\n","1      32\n","12      2\n","Name: count, dtype: int64\n","Batch - Images: torch.Size([64, 3, 224, 224]), Genres: torch.Size([64, 161])\n"]}]},{"cell_type":"code","source":["# ===========================\n","#      Model Definition\n","# ===========================\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomResNet, self).__init__()\n","        base_model = models.resnet34(pretrained=True)\n","        self.base = nn.Sequential(*list(base_model.children())[:-1])\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5),\n","        )\n","        self.output_layer = nn.Linear(256, num_classes)\n","\n","    def forward(self, x):\n","        x = self.base(x).flatten(1)\n","        x = self.fc_layers(x)\n","        return self.output_layer(x)\n","\n","model = CustomResNet(num_classes=len(parquet_dataset.genre_columns)).to('cuda' if torch.cuda.is_available() else 'cpu')\n"],"metadata":{"id":"8PhWP3_Xe1aJ","executionInfo":{"status":"ok","timestamp":1732584884937,"user_tz":300,"elapsed":636,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c854a9a5-bd2f-4fbd-b5d5-012e2e316723"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["# ===========================\n","#      Loss and Optimizer\n","# ===========================\n","criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n","optimizer = optim.AdamW(model.parameters(), lr=0.0007)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)\n"],"metadata":{"id":"8agE672d2il-","executionInfo":{"status":"ok","timestamp":1732584884937,"user_tz":300,"elapsed":8,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e821ec15-3be0-43e5-8a96-dc9d5e105351"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["print(f\"Full dataset length: {len(parquet_dataset)}\")\n","print(f\"Train dataset length: {len(train_idx)}, Validation dataset length: {len(val_idx)}\")\n","\n","print(f\"Max train index: {max(train_idx)}, Max dataset index: {len(parquet_dataset) - 1}\")\n","print(f\"Train indices: {train_idx[:10]}\")  # Print a few indices\n","\n","for images, genres in train_loader:\n","    print(images.shape, genres.shape)\n","    break\n"],"metadata":{"id":"L6z-HNrCt0Bq","executionInfo":{"status":"ok","timestamp":1732584885795,"user_tz":300,"elapsed":864,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dce9dd25-fc35-4790-b64a-5d7824d57530"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Full dataset length: 995\n","Train dataset length: 796, Validation dataset length: 199\n","Max train index: 994, Max dataset index: 994\n","Train indices: [112, 597, 787, 560, 817, 287, 705, 358, 57, 869]\n","torch.Size([64, 3, 224, 224]) torch.Size([64, 161])\n"]}]},{"cell_type":"code","source":["# ===========================\n","#      Training Loop\n","# ===========================\n","num_epochs = 30\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Initialize tracking variables for the best model\n","best_val_loss = float('inf')\n","best_epoch = -1\n","checkpoint_path = '/content/drive/MyDrive/Projects/movie_posters/Training/Models/best_model.pth'\n","\n","train_losses, val_losses = [], []\n","train_f1_scores, val_f1_scores = [], []\n","train_precisions, val_precisions = [], []\n","train_recalls, val_recalls = [], []\n","train_accuracies, val_accuracies = [], []\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss, train_correct, train_total = 0.0, 0, 0\n","    train_preds, train_labels = [], []\n","    for images, genres in train_loader:\n","        images, genres = images.to(device), genres.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, genres)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","        # Predictions and accuracy\n","        preds = (torch.sigmoid(outputs) > 0.5).int()\n","        train_correct += (preds == genres).all(dim=1).sum().item()\n","        train_total += genres.size(0)\n","        train_preds.append(torch.sigmoid(outputs).detach().cpu())\n","        train_labels.append(genres.cpu())\n","\n","    train_preds = torch.cat(train_preds)\n","    train_labels = torch.cat(train_labels)\n","    train_f1 = f1_score(train_labels.numpy(), (train_preds.numpy() > 0.5).astype(int), average='samples')\n","    train_precision = precision_score(train_labels.numpy(), (train_preds.numpy() > 0.5).astype(int), average='samples', zero_division=0)\n","    train_recall = recall_score(train_labels.numpy(), (train_preds.numpy() > 0.5).astype(int), average='samples')\n","\n","    train_losses.append(running_loss / len(train_loader))\n","    train_accuracies.append(train_correct / train_total)\n","    train_f1_scores.append(train_f1)\n","    train_precisions.append(train_precision)\n","    train_recalls.append(train_recall)\n","\n","    # Validation\n","    model.eval()\n","    val_running_loss, val_correct, val_total = 0.0, 0, 0\n","    val_preds, val_labels = [], []\n","    with torch.no_grad():\n","        for images, genres in val_loader:\n","            images, genres = images.to(device), genres.to(device)\n","            outputs = model(images)\n","            val_running_loss += criterion(outputs, genres).item()\n","\n","            preds = (torch.sigmoid(outputs) > 0.5).int()\n","            val_correct += (preds == genres).all(dim=1).sum().item()\n","            val_total += genres.size(0)\n","            val_preds.append(torch.sigmoid(outputs).cpu())\n","            val_labels.append(genres.cpu())\n","\n","    val_preds = torch.cat(val_preds)\n","    val_labels = torch.cat(val_labels)\n","    val_f1 = f1_score(val_labels.numpy(), (val_preds.numpy() > 0.5).astype(int), average='samples')\n","    val_precision = precision_score(val_labels.numpy(), (val_preds.numpy() > 0.5).astype(int), average='samples', zero_division=0)\n","    val_recall = recall_score(val_labels.numpy(), (val_preds.numpy() > 0.5).astype(int), average='samples')\n","\n","    val_losses.append(val_running_loss / len(val_loader))\n","    val_accuracies.append(val_correct / val_total)\n","    val_f1_scores.append(val_f1)\n","    val_precisions.append(val_precision)\n","    val_recalls.append(val_recall)\n","\n","    # Scheduler step\n","    scheduler.step(val_losses[-1])\n","\n","    # Check if this is the best model so far\n","    if val_losses[-1] < best_val_loss:\n","        best_val_loss = val_losses[-1]\n","        best_epoch = epoch + 1\n","        torch.save(model, checkpoint_path)\n","        print(f\"✅ Saved Best Model at Epoch {epoch+1} with Validation Loss: {best_val_loss:.4f}\")\n","    else:\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_losses[-1]:.4f}, \"\n","              f\"Val Loss: {val_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, \"\n","              f\"Val Acc: {val_accuracies[-1]:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\")\n","\n","print(f\"Training Complete! Best Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch}\")\n"],"metadata":{"id":"eFzU9dwoSmS7","executionInfo":{"status":"ok","timestamp":1732584965081,"user_tz":300,"elapsed":79289,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc3ce7c2-88ac-44dc-d5fb-5caeaeabe145"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved Best Model at Epoch 1 with Validation Loss: 1.3861\n","Epoch [2/30] - Train Loss: 2.3461, Val Loss: 1.4565, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0867, Val F1: 0.0420\n","Epoch [3/30] - Train Loss: 2.0731, Val Loss: 1.7698, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0768, Val F1: 0.0364\n","Epoch [4/30] - Train Loss: 1.9461, Val Loss: 1.5060, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0783, Val F1: 0.0409\n","Epoch [5/30] - Train Loss: 1.9114, Val Loss: 1.4474, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0845, Val F1: 0.0424\n","Epoch [6/30] - Train Loss: 1.8442, Val Loss: 1.4249, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0806, Val F1: 0.0430\n","✅ Saved Best Model at Epoch 7 with Validation Loss: 1.3802\n","Epoch [8/30] - Train Loss: 1.8077, Val Loss: 1.3952, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0844, Val F1: 0.0452\n","Epoch [9/30] - Train Loss: 1.8409, Val Loss: 1.4185, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0861, Val F1: 0.0461\n","Epoch [10/30] - Train Loss: 1.8060, Val Loss: 1.4557, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0858, Val F1: 0.0460\n","Epoch [11/30] - Train Loss: 1.8069, Val Loss: 1.6822, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0862, Val F1: 0.0429\n","Epoch [12/30] - Train Loss: 1.7906, Val Loss: 1.4457, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0868, Val F1: 0.0430\n","Epoch [13/30] - Train Loss: 1.8155, Val Loss: 1.3850, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0852, Val F1: 0.0450\n","✅ Saved Best Model at Epoch 14 with Validation Loss: 1.3714\n","✅ Saved Best Model at Epoch 15 with Validation Loss: 1.3689\n","✅ Saved Best Model at Epoch 16 with Validation Loss: 1.3684\n","Epoch [17/30] - Train Loss: 1.7483, Val Loss: 1.3701, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0878, Val F1: 0.0445\n","Epoch [18/30] - Train Loss: 1.7202, Val Loss: 1.3715, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0861, Val F1: 0.0448\n","Epoch [19/30] - Train Loss: 1.7470, Val Loss: 1.3707, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0897, Val F1: 0.0433\n","Epoch [20/30] - Train Loss: 1.7477, Val Loss: 1.3709, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0921, Val F1: 0.0432\n","Epoch [21/30] - Train Loss: 1.6992, Val Loss: 1.3711, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0896, Val F1: 0.0432\n","Epoch [22/30] - Train Loss: 1.7637, Val Loss: 1.3720, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0919, Val F1: 0.0435\n","Epoch [23/30] - Train Loss: 1.7396, Val Loss: 1.3733, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0915, Val F1: 0.0436\n","Epoch [24/30] - Train Loss: 1.7069, Val Loss: 1.3713, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0877, Val F1: 0.0435\n","Epoch [25/30] - Train Loss: 1.6996, Val Loss: 1.3725, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0863, Val F1: 0.0435\n","Epoch [26/30] - Train Loss: 1.7685, Val Loss: 1.3723, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0933, Val F1: 0.0435\n","Epoch [27/30] - Train Loss: 1.7102, Val Loss: 1.3729, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0872, Val F1: 0.0436\n","Epoch [28/30] - Train Loss: 1.7196, Val Loss: 1.3706, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0927, Val F1: 0.0434\n","Epoch [29/30] - Train Loss: 1.7195, Val Loss: 1.3721, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0913, Val F1: 0.0435\n","Epoch [30/30] - Train Loss: 1.7231, Val Loss: 1.3731, Train Acc: 0.0000, Val Acc: 0.0000, Train F1: 0.0875, Val F1: 0.0435\n","Training Complete! Best Validation Loss: 1.3684 at Epoch 16\n"]}]},{"cell_type":"code","source":["# ===========================\n","#        Plot Results\n","# ===========================\n","epochs = range(1, num_epochs + 1)\n","\n","plt.figure(figsize=(20, 15))\n","metrics = [\n","    (\"Train Loss\", \"Validation Loss\", train_losses, val_losses, \"Loss\"),\n","    (\"Train F1\", \"Validation F1\", train_f1_scores, val_f1_scores, \"F1 Score\"),\n","    (\"Train Precision\", \"Validation Precision\", train_precisions, val_precisions, \"Precision\"),\n","    (\"Train Recall\", \"Validation Recall\", train_recalls, val_recalls, \"Recall\")\n","]\n","\n","for i, (train_label, val_label, train_metric, val_metric, title) in enumerate(metrics, 1):\n","    plt.subplot(3, 2, i)\n","    plt.plot(epochs, train_metric, label=train_label, color='blue')\n","    plt.plot(epochs, val_metric, label=val_label, color='orange')\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(title)\n","    plt.title(f\"{title} over Epochs\")\n",