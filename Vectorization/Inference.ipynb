{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"19FXd5awShmjpVv-5IRrbPUdY2RAj3NPR","authorship_tag":"ABX9TyN0iGyaGH5DSAAR0R9r3Bqe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":59,"metadata":{"id":"p1WFacDmNxVt","executionInfo":{"status":"ok","timestamp":1732588774704,"user_tz":300,"elapsed":96,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms, models\n","from PIL import Image\n","import os\n","from google.colab import drive\n","from torch.utils.data import DataLoader, Dataset\n","import pandas as pd\n","import numpy as np\n","import io\n","from tqdm import tqdm  # For progress bar\n"]},{"cell_type":"code","source":["# ===========================\n","#    Mount Google Drive\n","# ===========================\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K_poPOKjP7Ud","executionInfo":{"status":"ok","timestamp":1732588776499,"user_tz":300,"elapsed":1692,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"outputId":"7394ad49-640a-4822-c6eb-cfdff421d12a"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["class ParquetDataset(Dataset):\n","    def __init__(self, parquet_file_or_df, transform=None):\n","        if isinstance(parquet_file_or_df, str):\n","            self.df = pd.read_parquet(parquet_file_or_df)\n","        elif isinstance(parquet_file_or_df, pd.DataFrame):\n","            self.df = parquet_file_or_df\n","        else:\n","            raise ValueError(\"parquet_file_or_df must be either a file path (str) or a pandas DataFrame.\")\n","\n","        self.transform = transform\n","\n","        # Identify genre columns by excluding known columns\n","        self.non_genre_columns = ['movie_id', 'movie_name', 'movie_poster']\n","        self.genre_columns = self.df.columns.drop(self.non_genre_columns)\n","        self.num_genres = len(self.genre_columns)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","\n","        # Load image from binary data\n","        image_binary = row['movie_poster']\n","        image = self.load_image_from_binary(image_binary)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Extract movie_id\n","        movie_id = row['movie_id']\n","\n","        return movie_id, image\n","\n","    @staticmethod\n","    def load_image_from_binary(image_binary):\n","        img_byte_arr = io.BytesIO(image_binary)\n","        image = Image.open(img_byte_arr).convert(\"RGB\")\n","        return image\n"],"metadata":{"id":"M4HTR2DLUGl7","executionInfo":{"status":"ok","timestamp":1732588776499,"user_tz":300,"elapsed":6,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#      Model Definition\n","# ===========================\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomResNet, self).__init__()\n","        base_model = models.resnet34(pretrained=True)\n","        self.base = nn.Sequential(*list(base_model.children())[:-1])  # Remove the final classification layer\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 512), nn.ReLU(), nn.Dropout(0.5),\n","            nn.Linear(512, 256), #Vector is this layer.\n","            nn.ReLU(), nn.Dropout(0.5),\n","        )\n","        self.output_layer = nn.Linear(256, num_classes)\n","\n","    def forward(self, x):\n","        x = self.base(x).flatten(1)  # Extract features\n","        x = self.fc_layers(x)        # Vectorization layers\n","        return self.output_layer(x)  # Classification layer\n","\n","    def get_vector(self, x):\n","      x = self.base(x).flatten(1)\n","      for layer in self.fc_layers:\n","          x = layer(x)\n","          if isinstance(layer, nn.Linear) and layer.out_features == 256:\n","              break  # Stop after the Linear(512, 256) layer\n","      return x"],"metadata":{"id":"-uWEdS1COAEj","executionInfo":{"status":"ok","timestamp":1732588776500,"user_tz":300,"elapsed":6,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#      Model Loading\n","# ===========================\n","def load_model(checkpoint_path, num_classes, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","    \"\"\"\n","    Loads the trained CustomResNet model from the checkpoint.\n","\n","    Parameters:\n","        checkpoint_path (str): Path to the saved model checkpoint.\n","        num_classes (int): Number of output classes.\n","        device (str): Device to load the model on ('cuda' or 'cpu').\n","\n","    Returns:\n","        CustomResNet: Loaded model in evaluation mode.\n","    \"\"\"\n","    if not os.path.exists(checkpoint_path):\n","        raise FileNotFoundError(f\"Checkpoint not found at '{checkpoint_path}'\")\n","\n","    # Initialize the model architecture\n","    model = CustomResNet(num_classes=num_classes)\n","\n","    # Load the state_dict\n","    torch.load(checkpoint_path, map_location=device)\n","    model.to(device)\n","    model.eval()  # Set to evaluation mode\n","    return model"],"metadata":{"id":"OSXk6bs5OD-o","executionInfo":{"status":"ok","timestamp":1732588776500,"user_tz":300,"elapsed":6,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#          Transforms\n","# ===========================\n","def get_transform():\n","    \"\"\"\n","    Returns the image transformations used during training.\n","\n","    Returns:\n","        torchvision.transforms.Compose: Composed transformations.\n","    \"\"\"\n","    return transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n","                             std=[0.229, 0.224, 0.225]),  # ImageNet std\n","    ])"],"metadata":{"id":"ZejZi4x2OHSG","executionInfo":{"status":"ok","timestamp":1732588776500,"user_tz":300,"elapsed":5,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#    Inference Function\n","# ===========================\n","def get_vector_from_image(image_input, model, transform, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","    \"\"\"\n","    Processes the input image and returns a 256-dimensional feature vector.\n","\n","    Parameters:\n","        image_input (str or PIL.Image.Image): Path to the image file or a PIL Image object.\n","        model (CustomResNet): The trained model.\n","        transform (torchvision.transforms.Compose): Preprocessing transformations.\n","        device (str): Device to perform computation on.\n","\n","    Returns:\n","        numpy.ndarray: 256-dimensional feature vector.\n","    \"\"\"\n","    # Load the image\n","    if isinstance(image_input, str):\n","        if not os.path.exists(image_input):\n","            raise FileNotFoundError(f\"Image file not found at '{image_input}'\")\n","        image = Image.open(image_input).convert(\"RGB\")\n","    elif isinstance(image_input, Image.Image):\n","        image = image_input.convert(\"RGB\")\n","    else:\n","        raise ValueError(\"image_input must be a file path or a PIL.Image.Image object.\")\n","\n","    # Apply transformations\n","    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n","\n","    with torch.no_grad():\n","        vector = model.get_vector(image)  # Get the 256-dimensional vector\n","        vector = vector.cpu().numpy().flatten()\n","\n","    return vector"],"metadata":{"id":"4RZVHRlROKmw","executionInfo":{"status":"ok","timestamp":1732588776500,"user_tz":300,"elapsed":5,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# ===========================\n","#         Usage Example\n","# ===========================\n","if __name__ == \"__main__\":\n","    # Path to the saved model checkpoint\n","    checkpoint_path = '/content/drive/MyDrive/Projects/movie_posters/Training/Models/best_model.pth'\n","\n","    # Number of classes (replace with your actual number of genre columns)\n","    num_classes = 20  # Example value; replace with your actual number\n","\n","    # Initialize device\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    print(f\"Using device: {device}\")\n","\n","    # Load the trained model\n","    try:\n","        model = load_model(checkpoint_path, num_classes=num_classes, device=device)\n","        print(\"Model loaded successfully.\")\n","    except Exception as e:\n","        print(f\"Error loading model: {e}\")\n","        exit(1)\n","\n","    # Get the transformation pipeline\n","    transform = get_transform()\n","\n","    # Path to the input image for inference\n","    image_path = '/content/drive/MyDrive/Projects/movie_posters/Vectorization/poster_image.jpg'  # Replace with your image path\n","\n","    # Check if image exists\n","    if not os.path.exists(image_path):\n","        print(f\"Image file not found at '{image_path}'\")\n","        exit(1)\n","\n","    # Get the 256-dimensional vector\n","    try:\n","        vector = get_vector_from_image(image_path, model, transform, device=device)\n","        print(f\"Vector shape: {vector.shape}\")\n","        print(f\"Vector: {vector}\")\n","    except Exception as e:\n","        print(f\"Error during inference: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w7nV4G6FN70S","executionInfo":{"status":"ok","timestamp":1732588777186,"user_tz":300,"elapsed":690,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"outputId":"c05e9707-9c40-41f4-cd42-4240de6238ce"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","<ipython-input-63-e73dcea7135c>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  torch.load(checkpoint_path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded successfully.\n","Vector shape: (256,)\n","Vector: [ 0.05856618  0.03154264 -0.02973773  0.02552691  0.00126035  0.00360497\n"," -0.02882855  0.03062486 -0.02029199 -0.03287343  0.0408044   0.02266661\n","  0.0383857  -0.00882797  0.02130431 -0.00712389 -0.03790253 -0.01565099\n"," -0.01244076 -0.01609455  0.04103363 -0.03976864 -0.0060688  -0.04436138\n"," -0.00957141 -0.03470837  0.00668672 -0.02438618  0.00188775 -0.06013023\n"," -0.030621    0.0517728   0.04189692 -0.00377822 -0.01050789 -0.02665899\n"," -0.04136047 -0.00627694 -0.00513279 -0.03514675 -0.03070272 -0.03292549\n","  0.04396499  0.01870156 -0.03442657 -0.01046899  0.03248999 -0.03566707\n","  0.02777395 -0.00137688 -0.04080449 -0.01196069 -0.00493136  0.006618\n"," -0.02525345  0.03621399  0.04613086 -0.04903983  0.02785776 -0.02644226\n"," -0.03053804 -0.01671258 -0.02704752 -0.03511086  0.0260879  -0.02371722\n"," -0.0286771   0.03857456 -0.02903061 -0.03686766 -0.03618192 -0.00799718\n","  0.03343593  0.02720884 -0.03677804  0.001142    0.0043915   0.0226765\n","  0.01019962 -0.02019678  0.02463567 -0.01351115 -0.03870969  0.01790183\n"," -0.00741011 -0.00077054  0.00443303 -0.05347365  0.03959275 -0.00628944\n","  0.03353611  0.02333237 -0.01971457 -0.01296947 -0.04053626 -0.02877162\n","  0.00911122 -0.03670903  0.03193709  0.02516006 -0.01507731  0.01903657\n","  0.02131081  0.04035554 -0.02744867 -0.0249868   0.01424184  0.05213104\n"," -0.00623027 -0.00660698 -0.01459503  0.00167226 -0.03142839 -0.00650967\n","  0.0116468   0.01103222 -0.02694191  0.0137612  -0.02000495  0.05317693\n","  0.01746296 -0.02086535 -0.01623377  0.03743038 -0.03311506  0.01182717\n","  0.00543078 -0.02201249  0.0505093   0.01671856  0.01707056  0.03629142\n"," -0.00854985 -0.00271089  0.02798045 -0.0081457   0.00209052  0.00551879\n","  0.03637714  0.0011088  -0.00627554  0.00432617  0.01265984 -0.02345365\n"," -0.00646095  0.00027813  0.01576296  0.02001629  0.0217029  -0.03513531\n"," -0.02308705  0.01909314 -0.02550804  0.01207476 -0.01353002  0.01058762\n","  0.01602139 -0.0162246   0.00165876 -0.03181531  0.02324151 -0.02116514\n","  0.0151912  -0.03358786 -0.01815365  0.01270699 -0.02473888 -0.01051918\n"," -0.01511732  0.03811029  0.00096858  0.00332456 -0.02647245 -0.02483204\n"," -0.01616829  0.02915426 -0.02192949 -0.04930133  0.02002943 -0.02418206\n","  0.00124294  0.0382242  -0.03377245 -0.01816327  0.02460763  0.02170933\n"," -0.00935877  0.0320558   0.04410267  0.02866342  0.00587927  0.02015846\n","  0.00014764  0.01518087  0.00767084  0.0036255  -0.04204743  0.00871701\n"," -0.03208687  0.02042428 -0.01632167  0.01933306  0.00479166 -0.05498781\n","  0.01770798  0.04693694  0.04956856  0.00945889  0.03010053  0.0279203\n"," -0.02201326  0.02131132  0.02612361 -0.02184544 -0.01995972 -0.00376598\n"," -0.01065647 -0.01277397 -0.00185832  0.01402282 -0.01190566 -0.02415521\n","  0.04341809 -0.0034809  -0.006441    0.00017255 -0.00857108 -0.05073618\n","  0.03888125 -0.01828057  0.0372896   0.01651523  0.01426322  0.00758506\n","  0.00721853  0.04605329  0.04107326  0.02621468 -0.02609388  0.03093167\n"," -0.02799727 -0.02865534  0.00627686 -0.01543638 -0.00637616  0.03885823\n"," -0.0235115   0.04092639 -0.00713753 -0.00013592  0.05269641 -0.00169714\n","  0.02415435  0.00897153 -0.00735036  0.00625264]\n"]}]},{"cell_type":"code","source":["def vectorize_dataset(parquet_path, model_checkpoint_path, output_csv_path, batch_size=64, num_workers=4, device='cuda' if torch.cuda.is_available() else 'cpu'):\n","    \"\"\"\n","    Processes all images in the parquet dataset through the model's vectorization layer\n","    and saves the movie_id and corresponding 256-dimensional vectors to a CSV file.\n","\n","    Parameters:\n","        parquet_path (str): Path to the parquet file containing the dataset.\n","        model_checkpoint_path (str): Path to the saved model state dictionary.\n","        output_csv_path (str): Path to save the output CSV file.\n","        batch_size (int): Number of samples per batch for DataLoader.\n","        num_workers (int): Number of subprocesses to use for data loading.\n","        device (str): Device to perform computation on ('cuda' or 'cpu').\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Define transforms\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n","                             std=[0.229, 0.224, 0.225]),  # ImageNet std\n","    ])\n","\n","    # Initialize the dataset\n","    print(\"Loading dataset...\")\n","    dataset = ParquetDataset(parquet_path, transform=transform)\n","    print(f\"Dataset loaded with {len(dataset)} samples.\")\n","\n","    # Initialize the model\n","    num_classes = dataset.num_genres\n","    model = CustomResNet(num_classes=num_classes)\n","\n","    # Load the state dictionary\n","    print(\"Loading model state dictionary...\")\n","    try:\n","        model = torch.load(model_checkpoint_path, map_location=device)\n","        print(\"Model state dictionary loaded successfully.\")\n","    except Exception as e:\n","        print(f\"Error loading model state dictionary: {e}\")\n","        return\n","\n","    model.to(device)\n","    model.eval()  # Set model to evaluation mode\n","\n","    # Create DataLoader\n","    print(\"Creating DataLoader...\")\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","    # Initialize lists to store movie_ids and vectors\n","    all_movie_ids = []\n","    all_vectors = []\n","\n","    # Iterate through DataLoader with a progress bar\n","    print(\"Starting vectorization process...\")\n","    for batch_movie_ids, images in tqdm(data_loader, desc=\"Vectorizing Images\"):\n","        images = images.to(device)\n","\n","        with torch.no_grad():\n","            vectors = model.get_vector(images)  # Get the 256-dimensional vectors\n","            vectors = vectors.cpu().numpy()     # Move to CPU and convert to NumPy array\n","\n","        all_movie_ids.extend(batch_movie_ids)\n","        all_vectors.extend(vectors)\n","\n","    # Convert vectors to a DataFrame\n","    print(\"Converting vectors to DataFrame...\")\n","    vectors_np = np.array(all_vectors)  # Shape: (num_samples, 256)\n","    vector_columns = [f'vec_{i}' for i in range(vectors_np.shape[1])]\n","\n","    df_vectors = pd.DataFrame(vectors_np, columns=vector_columns)\n","    df_vectors.insert(0, 'movie_id', all_movie_ids)\n","\n","    # Save to CSV\n","    print(f\"Saving vectors to {output_csv_path}...\")\n","    df_vectors.to_csv(output_csv_path, index=False)\n","    print(f\"Saved vectors to {output_csv_path}\")\n"],"metadata":{"id":"m-VGc8pIT-hL","executionInfo":{"status":"ok","timestamp":1732588777186,"user_tz":300,"elapsed":4,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# Define paths\n","parquet_file_path = '/content/drive/MyDrive/Projects/movie_posters/Training/dataset.parquet'  # Update as needed\n","checkpoint_path = '/content/drive/MyDrive/Projects/movie_posters/Training/Models/best_model.pth'  # Update as needed\n","output_csv = '/content/drive/MyDrive/Projects/movie_posters/Vectorization/Vectors/Vectors.csv'  # Desired output path\n","\n","# Run the vectorization process\n","vectorize_dataset(\n","    parquet_path=parquet_file_path,\n","    model_checkpoint_path=checkpoint_path,\n","    output_csv_path=output_csv,\n","    batch_size=64,       # Adjust based on your GPU/CPU memory\n","    num_workers=4,       # Adjust based on your system\n","    device='cuda'        # Ensure CUDA is available, else use 'cpu'\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vAicajAURNp","executionInfo":{"status":"ok","timestamp":1732588824129,"user_tz":300,"elapsed":2306,"user":{"displayName":"John Guinness","userId":"14687229214683856304"}},"outputId":"5b6ef9f6-86ec-4eea-934f-ed386ff698ae"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Dataset loaded with 994 samples.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","<ipython-input-67-00b8b4f9d100>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_checkpoint_path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["Loading model state dictionary...\n","Model state dictionary loaded successfully.\n","Creating DataLoader...\n","Starting vectorization process...\n"]},{"output_type":"stream","name":"stderr","text":["Vectorizing Images: 100%|██████████| 16/16 [00:01<00:00, 13.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Converting vectors to DataFrame...\n","Saving vectors to /content/drive/MyDrive/Projects/movie_posters/Vectorization/Vectors/Vectors.csv...\n","Saved vectors to /content/drive/MyDrive/Projects/movie_posters/Vectorization/Vectors/Vectors.csv\n"]}]}]}